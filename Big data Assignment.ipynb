{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case: Optimering af Online Retail Platform**\n",
    "\n",
    "Du arbejder som en dataingeniør i en online detailhandelsvirksomhed ved navn \"X,\" der ønsker at forbedre sin kundeservice og salgsstrategi ved at udnytte Big Data. Virksomheden har adgang til en bred vifte af datakilder, herunder transaktionsdata, brugeradfærd på webstedet, sociale mediedata og eksterne markedsdata. Din opgave er at designe og opbygge en mindre Big Data løsning for X.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Dataindsamling og Opsamling:**\n",
    "   - Design en løsning til at indsamle og opsamle data fra forskellige kilder i realtid, inklusive transaktionsdata, webadfærd og sociale mediedata. Overvej anvendelse af HTTPS (REST), Kafka eller MQTT til dataindsamling.\n",
    "   - Sikkerhedsaspekter er vigtige. Implementer kryptering af data under transport for at beskytte følsomme oplysninger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert storageDrive.csv to Microsoft Sql Server database\n",
    "\n",
    "import csv\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "df = pd.read_csv('walmart.csv')\n",
    "#Print available drivers\n",
    "#print(pyodbc.drivers())\n",
    "\n",
    "import urllib\n",
    "import getpass\n",
    "\n",
    "# user = getpass.getuser()\n",
    "#Promt for password\n",
    "password = getpass.getpass()\n",
    "\n",
    "params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};\" + f'SERVER=tcp:nextchsolutions.database.windows.net,1433;DATABASE=NexTechSolutions;UID=ktfs;PWD={password};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=60;')\n",
    "engine = create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params,fast_executemany=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Put the 'raw data' into a SQL database\n",
    "df.to_sql('WalmartDb', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Dataforberedelse og Transformation:**\n",
    "   - Opbyg en pipeline til at rengøre, strukturere og forberede data til analyse. Dette inkluderer valg af filformater som CSV, XML, HDF5 osv.\n",
    "   - Håndter forskellige datatyper, herunder struktureret og ustruktureret data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if LoadDataTask() is complete\n",
      "c:\\Users\\ssc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\luigi\\worker.py:419: UserWarning:\n",
      "\n",
      "Task LoadDataTask() without outputs has no custom complete() method\n",
      "\n",
      "DEBUG: Checking if TransformDataTask() is complete\n",
      "INFO: Informed scheduler that task   LoadDataTask__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   TransformDataTask__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 3288] Worker Worker(salt=2667688117, workers=1, host=UIT-MJ2rtjbJ8yG, username=ssc, pid=3288) running   LoadDataTask()\n",
      "INFO: [pid 3288] Worker Worker(salt=2667688117, workers=1, host=UIT-MJ2rtjbJ8yG, username=ssc, pid=3288) done      LoadDataTask()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   LoadDataTask__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=2667688117, workers=1, host=UIT-MJ2rtjbJ8yG, username=ssc, pid=3288) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 2 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 TransformDataTask()\n",
      "* 1 ran successfully:\n",
      "    - 1 LoadDataTask()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import luigi\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sqlalchemy import create_engine, types\n",
    "import os\n",
    "import urllib\n",
    "import getpass\n",
    "\n",
    "# user = getpass.getuser()\n",
    "#Promt for password\n",
    "password = getpass.getpass()\n",
    "\n",
    "params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};\" + f'SERVER=tcp:nextchsolutions.database.windows.net,1433;DATABASE=NexTechSolutions;UID=ktfs;PWD={password};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=60;')\n",
    "engine = create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params,fast_executemany=True)\n",
    "#engine = create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params)\n",
    "\n",
    "class ExtractDataTask(luigi.Task):\n",
    "    #The task to run\n",
    "    def run(self):\n",
    "        # Check if the file exists and delete it if it does\n",
    "        if os.path.exists('walmart_extracted.csv'):\n",
    "            os.remove('walmart_extracted.csv')\n",
    "        \n",
    "        conn = pyodbc.connect(engine)\n",
    "        query = '''\n",
    "            SELECT * FROM WalmartDb\n",
    "        '''\n",
    "        df = pd.read_sql(query, conn)\n",
    "        df.to_csv('walmart_extracted.csv', index=False)\n",
    "        conn.close()\n",
    "    #The output of the task\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget('walmart_extracted.csv')\n",
    "\n",
    "\n",
    "class TransformDataTask(luigi.Task):\n",
    "    #The task this task is dependent on, the task will not run unless the task defined in requires() has completed successfully.\n",
    "    def requires(self):\n",
    "        return ExtractDataTask()\n",
    "\n",
    "    def run(self):\n",
    "        # Check if the file exists and delete it if it does\n",
    "        if os.path.exists('walmart_transformed.csv'):\n",
    "            os.remove('walmart_transformed.csv')\n",
    "\n",
    "        input_file = self.input().path\n",
    "        df = pd.read_csv(input_file)\n",
    "        df = df.drop_duplicates()\n",
    "        df = df.dropna()\n",
    "        df.to_csv('walmart_transformed.csv', index=False)\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget('walmart_transformed.csv')\n",
    "\n",
    "\n",
    "class LoadDataTask(luigi.Task):\n",
    "    def requires(self):\n",
    "        return TransformDataTask()\n",
    "\n",
    "    def run(self):\n",
    "        input_file = self.input().path\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        column_types = {col: sql_type for col, sql_type in zip(df.columns, df.dtypes)}\n",
    "\n",
    "        column_types['Store'] = types.BigInteger()\n",
    "        column_types['Date'] = types.String()\n",
    "        column_types['Weekly_Sales'] = types.FLOAT()\n",
    "        column_types['Holiday_Flag'] = types.BigInteger()\n",
    "        column_types['Temperature'] = types.Float()\n",
    "        column_types['Fuel_Price'] = types.Float()\n",
    "        column_types['CPI'] = types.Float()\n",
    "        column_types['Unemployment'] = types.Float()\n",
    "        df.to_sql('WalmartDb_clean', engine, if_exists='replace', index=False,dtype=column_types)\n",
    "\n",
    "# Run the pipeline\n",
    "luigi.build([LoadDataTask()], local_scheduler=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Analyse og Præsentation:**\n",
    "   - Anvend dataanalyseværktøjer og teknikker til at trække indsigt fra de forberedte data.\n",
    "   - Design dashboards eller rapporter for at give virksomhedens beslutningstagere nem adgang til relevante oplysninger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 1 must be a string or unicode object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\SkoleRepos\\ThiccData\\PythonThiccData\\Big data Assignment.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/SkoleRepos/ThiccData/PythonThiccData/Big%20data%20Assignment.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m password \u001b[39m=\u001b[39m getpass\u001b[39m.\u001b[39mgetpass()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SkoleRepos/ThiccData/PythonThiccData/Big%20data%20Assignment.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m conn_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDRIVER=\u001b[39m\u001b[39m{\u001b[39m\u001b[39mODBC Driver 17 for SQL Server};\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSERVER=tcp:nextchsolutions.database.windows.net,1433;DATABASE=NexTechSolutions;UID=ktfs;PWD=\u001b[39m\u001b[39m{\u001b[39;00mpassword\u001b[39m}\u001b[39;00m\u001b[39m;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=60;\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/SkoleRepos/ThiccData/PythonThiccData/Big%20data%20Assignment.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m conn \u001b[39m=\u001b[39m pyodbc\u001b[39m.\u001b[39;49mconnect(engine)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SkoleRepos/ThiccData/PythonThiccData/Big%20data%20Assignment.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SkoleRepos/ThiccData/PythonThiccData/Big%20data%20Assignment.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    SELECT * FROM WalmartDb_clean\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SkoleRepos/ThiccData/PythonThiccData/Big%20data%20Assignment.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m'''\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SkoleRepos/ThiccData/PythonThiccData/Big%20data%20Assignment.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_sql(query, conn)\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 1 must be a string or unicode object"
     ]
    }
   ],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dash\n",
    "import dash_html_components as html\n",
    "from PIL import Image\n",
    "\n",
    "password = getpass.getpass()\n",
    "conn_str = \"DRIVER={ODBC Driver 17 for SQL Server};\" + f'SERVER=tcp:nextchsolutions.database.windows.net,1433;DATABASE=NexTechSolutions;UID=ktfs;PWD={password};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=60;'\n",
    "conn = pyodbc.connect(conn_str)\n",
    "query = '''\n",
    "    SELECT * FROM WalmartDb_clean\n",
    "'''\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'],format='mixed')\n",
    "\n",
    "df['Day'] = df['Date'].dt.day\n",
    "\n",
    "g = sns.pairplot(data=df,x_vars=['Day','Temperature'],y_vars='Weekly_Sales',kind='scatter', plot_kws={'alpha':0.8, 'color': 'green'}, height=7)\n",
    "g.fig.suptitle('Weekly Sales vs  Day, Temperature', y=1.05)\n",
    "g.fig.savefig('pairplot.png')\n",
    "g = sns.pairplot(data=df,x_vars=['Fuel_Price','Unemployment'],y_vars='Weekly_Sales',kind='scatter', plot_kws={'alpha':0.8, 'color': 'green'}, height=7)\n",
    "g.fig.suptitle('Weekly Sales vs Fuel Price, Unemployment', y=1.05)\n",
    "g.fig.savefig('pairplot1.png')\n",
    "\n",
    "pil_image = Image.open(\"pairplot.png\")\n",
    "pil_image1 = Image.open(\"pairplot1.png\")\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "app.title = 'Dash Board'\n",
    "app.layout = html.Div(\n",
    "    children=[\n",
    "        \n",
    "        html.H1('DASH BOARD', style={'textAlign': 'center'}),\n",
    "        html.Div(children=[\n",
    "            html.Img(src=pil_image, alt='pairplot'),\n",
    "            html.Img(src=pil_image1, alt='pairplot')\n",
    "        ], style={'textAlign': 'center'})\n",
    "\n",
    "])\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Sikkerhed og Overholdelse:**\n",
    "   - Implementer omfattende sikkerhedsforanstaltninger for at beskytte data mod uautoriseret adgang og misbrug.\n",
    "   - Overhold databeskyttelsesregler, især GDPR, når du håndterer brugerdata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Skalering og Optimering:**\n",
    "   - Overvej hvordan løsningen kan skaleres i fremtiden med virksomhedens vækst og øgede datamængder.\n",
    "   - Identificer muligheder for ydeevneoptimering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Din Big Data løsning skal hjælpe RetailX med at træffe informerede beslutninger, forbedre kundeservicen og optimere salgsstrategien ved at udnytte de enorme datamængder, der er tilgængelige. Din evne til at integrere dataindsamling, forberedelse, analyse og sikkerhed vil være afgørende for succes i denne opgave."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
